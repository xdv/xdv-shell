// Shell Lexer - Tokenizer

forge ShellLexerErrors {
    const ERR_OK: UInt32 = 0;
    const ERR_INVALID_TOKEN: UInt32 = 1;
    const ERR_EOF: UInt32 = 2;
    const ERR_TOO_LONG: UInt32 = 3;
    const ERR_DOMAIN_NOT_AVAILABLE: UInt32 = 100;
}

forge ShellLexer {
    
    const TOKEN_WORD: UInt32 = 1;
    const TOKEN_PIPE: UInt32 = 2;
    const TOKEN_REDIR_IN: UInt32 = 3;
    const TOKEN_REDIR_OUT: UInt32 = 4;
    const TOKEN_REDIR_APPEND: UInt32 = 5;
    const TOKEN_EOF: UInt32 = 0;
    
    const MAX_TOKEN_LEN: UInt32 = 64;
    const MAX_TOKENS: UInt32 = 32;
    
    proc K::init(input: UInt64) -> UInt32 {
        if input == 0 {
            return ERR_EOF;
        } else {
            return ERR_OK;
        }
    }
    
    proc K::next_token() -> UInt32 {
        return TOKEN_WORD;
    }
    
    proc K::get_token_value() -> UInt64 {
        return 0;
    }
    
    proc K::peek_token() -> UInt32 {
        return TOKEN_WORD;
    }
    
    proc K::reset() -> UInt32 {
        return ERR_OK;
    }
    
    proc K::tokenize(input: UInt64, output: UInt64) -> UInt32 {
        let init_result = init(input);
        if init_result == ERR_OK {
            return next_token();
        } else {
            return init_result;
        }
    }
    
    
    
    
    
    
    
    
    
    
    
    
}



